---
title: "Lake Connectivity Analysis"
author: "Chris Madsen"
date: "'r Sys.Date()'"
output:  
prettydoc::html_pretty:
    theme: material
    highlight: github
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(ggthemes)
library(ggspatial)
library(patchwork)
library(rmapshaper)
library(tictoc)
library(bcdata)
rm(list = ls())
```

```{r load in data}
bc = read_sf("W:/CMadsen/SpatialData/bc_simple.shp")
#lakes = read_sf("W:/CMadsen/SpatialData/LakePoly.shp")
subw = bcdc_query_geodata('freshwater-atlas-watershed-groups') %>% 
  collect()
```

```{r set parameters}
regions = subw
starting_number = 1
```

```{r}
#Make table to store results for lake network assignments.
# if(file.exists("W:/CMadsen/WaterbodyConnectivity/lakes_network_table.csv")){
#   lake_table = read_csv("W:/CMadsen/WaterbodyConnectivity/lakes_network_table.csv")
# }else{
# lake_table = lakes %>% 
#   st_drop_geometry() %>% 
#   select(WATERBOD_1,WATERSHED_,GNIS_NAME_) %>% mutate(
#       network_id = NA, num_connections = NA)
# }
```

```{r background_plot}
# find_spatial_networks = function(polygon_of_interest,
#                                  connector,
#                                  regions,
#                                  id_field1 = "WATERBOD_1",
#                                  id_field2 = "WATERSHED_",
#                                  id_field3 = "GNIS_NAME_",
#                                  output_table,
#                                  starting_number = 1,
#                                  output_folder = "W:/CMadsen/WaterbodyConnectivity"){

#Set up visual output for user.
p_grid = ggplot() + 
  geom_sf(data = bc) +
  ggthemes::theme_map()

print(p_grid)
```

```{r analysis loop}
####################################
### Run analysis for each region!###
####################################

#If we're restarting the function at some point past i == 1...

if(file.exists("lakes_network_table.csv")){
  output_table = read_csv("lakes_network_table.csv")
}

for(i in starting_number:nrow(regions)){
  
  region = regions[i,] %>% summarise(subwatershed = WATERSHED_GROUP_ID)
    
  #Update visual "UI"
  updated_grid = p_grid + 
    geom_sf(data = region, col = "blue", fill = "lightblue", alpha = 0.5) + 
    labs(title = paste0("Region ",region$subwatershed,", ",i, " of ",nrow(regions)))
  
  print(updated_grid)
  
  #If the region is made up of a large number of separate polygons, remove any that are less than
  #1,000,000 square meters.
  if(st_geometry_type(region) == "MULTIPOLYGON"){
    region = region %>% 
      st_cast("POLYGON") %>% 
      mutate(area = as.numeric(st_area(.))) %>% 
      filter(area > 1000000) %>% 
      st_cast("MULTIPOLYGON")
  }
      
  #Load in streams and lakes for this subwatershed.
  
  print(paste0("Region ",i," - Downloading streams and lakes with bcdata package"))

  streams = bcdc_query_geodata('freshwater-atlas-stream-network') %>% filter(INTERSECTS(region)) %>% collect()
  lakes = bcdc_query_geodata('freshwater-atlas-lakes') %>% filter(INTERSECTS(region)) %>% collect()
  
  print(paste0("Region ",i," - Finished data download for this region"))
    
  streams = st_join(streams, region, st_intersects) %>% filter(!is.na(subwatershed))
  lakes = st_join(lakes, region, st_intersects) %>% filter(!is.na(subwatershed))
  
  print(paste0("Region ",i," - Cropped streams and lakes to region outline"))
  
  #Buffer streams
  tic()
  streams = st_buffer(streams, 1)
  #print("Buffering of connector shapefile complete.")
  toc()

  
  #Buffer polygon of interest by a little bit (3 meters) to ensure spatial overlap.
  tic()
  print("Buffering polygon of interest to ensure some overlap with connectors.")
  lakes = st_buffer(lakes, dist = 3)
  toc()
  
  print("Building networks.")
  
  #Prepare shapefiles for network analysis.
  lakes = lakes %>% 
    select(WATERBODY_KEY,WATERSHED_GROUP_ID,GNIS_NAME_1) %>% 
    mutate(num_components = 1)
  
  streams = streams %>% 
    select(geometry) %>% 
    st_zm()
    
  #Get number of streams / rivers connecting to lakes.
  number_connections_table = as.data.frame(st_intersects(lakes, streams)) %>% 
    as_tibble() %>% 
    group_by(row.id) %>% 
    summarise(number_connections = n())
  
  lakes$num_connections = 0
  
  lakes[number_connections_table$row.id,]$num_connections = number_connections_table$number_connections
  print("Found number of connections for each polygon of interest.")
    
  tic()
  print("Union and cast polygons to networks...")
  networks = st_cast(st_union(lakes %>% 
                                bind_rows(streams)), "POLYGON") %>% 
    st_as_sf()
  toc()
  
  #Find out which polygons from the cropped polygon of interest layer are 
  #in each network.
  tic()
  networks = networks %>% mutate(network_number = row_number(),
                        region = i)
    
  lakes = st_join(networks, lakes, st_intersects)
    
  #Just keep the largest network ID if a polygon of interest is in 2 networks...
  lakes = lakes %>% 
    st_drop_geometry() %>% 
    group_by(WATERBODY_KEY,
             WATERSHED_GROUP_ID,
             GNIS_NAME_1) %>% 
    arrange(num_components) %>% 
    slice(1)
  
  print(paste0("Network IDs assigned to polygons of interest in region ",i))
  toc()
    
  #Save the results of each loop to a table.
  
  lakes = lakes %>% 
    mutate(network_number = paste0(WATERSHED_GROUP_ID,"-",network_number))
  
  if(i == 1){
    output_table = lakes[0,] %>% ungroup()
    output_table$regions_completed = i
  }
  
  output_table = bind_rows(output_table, lakes %>% 
                             dplyr::select(-num_components) %>% 
                             ungroup())
  
  write.csv(output_table, 
              paste0("lakes_network_table.csv"),
              row.names = F)
  
  print(paste0("Lake network table updated. Number of non-NA lakes is now: ", nrow(output_table %>% filter(!is.na(network_number)))))
    
  print(paste0("Networks found for region ",i))
}

```

```{r}
lake_networks = output_table

#The network IDs are wonky... there are many missing numbers. Potentially, the networks were identified correctly but the numbering system is weird. If this is the case, I should be able to clean them up here.

networks_per_subw = lake_networks %>% 
  group_by(WATERSHED_GROUP_ID) %>% 
  summarise(number_networks_per_subw = length(unique(network_number))) %>% 
  mutate(network_correction_factor = lag(number_networks_per_subw)) %>% 
  mutate(network_correction_factor = replace_na(network_correction_factor, 0)) %>% 
  mutate(network_correction_factor = cumsum(network_correction_factor))

lake_networks = lake_networks %>% 
  filter(WATERSHED_GROUP_ID == 2) %>% 
  mutate(network_number = as.numeric(str_remove(network_number, "[0-9]*-"))) %>% 
  arrange(network_number) %>%
  mutate(network_number = as.factor(network_number)) %>% 
  mutate(network_number = fct_infreq(network_number)) %>% 
  mutate(network_number = as.numeric(network_number)) %>%
  #Add in the table showing the number of unique components in each subwatershed.
  left_join(networks_per_subw) %>% 
  mutate(network_number = network_number + network_correction_factor)
  
```

```{r}
#To stitch together any networks that SHOULD be a single network but were
#split apart by subwatershed boundaries, we can first find all networks that come close to subwatershed boundaries.

for(i in 1:nrow(subw)){
  
}

```


```{r}

# 1. 

lake_table = read_csv("W:/CMadsen/WaterbodyConnectivity/lakes_network_table.csv")

lake_n1 = lakes %>% 
  inner_join(lake_table %>% filter(network_id == 1))

ggplot() + geom_sf(data = lake_n1)
network_chunk_stitcher = function(x,
                                  area = "W:/CMadsen/SpatialData/bc_simple.shp",
                                  dist_to_boundary = 5000){

  
  if(file.exists(paste0(data_folder,"/area_squares.shp"))){
    area_squares = read_sf(paste0(data_folder,"/area_squares.shp"))
    print("Read in area of interest squares.")
  }else{
  warning("No 'area_squares' layer found in data folder")
  warning("Please run 'split_polygons_in_area' function first")
  break
  }
  
  #Make shapefile that represents boundaries between chunks.
  #Thickness of boundaries determined in function call.
  area_boundaries = ms_erase(area_squares, st_buffer(area_squares, dist = -dist_to_boundary))
  
  dat = st_join(dat, area_boundaries, st_intersects)
  
  dat_to_stitch = dat %>% filter(!is.na(some_field_name_from_gpkg))
  
  dat_dont_stitch = dat %>% filter(is.na(some_field_name_from_gpkg))
  
  rm(dat)
  
  #Do another dissolve and st_cast on "dat_to_stitch" to join networks that should actually be joined. Do a test one to make sure we have overlaps.
  networks_to_stitch = unique(dat_to_stitch$Some_ID_column)
  for(network in networks_to_stitch){
    
  }
  
  #Take the lower of the 2+ network ids and apply them to all of the 
  #components of any networks that have been found to be part of 
  #the same overall network.
  
  #Correct the lake table so that they have updated network ids.
  
  #Write out shapefile of networks
}
```

```{r}
network_chunk_stitcher(x = "lake_network_results.gpkg")
```

