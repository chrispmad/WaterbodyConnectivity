---
title: "Lake Connectivity Analysis"
author: "Chris Madsen"
date: "'r Sys.Date()'"
output:  
prettydoc::html_pretty:
    theme: material
    highlight: github
    df_print: kable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}

# ---------------------------------------------------------- #
# This script analyses all of the rivers and lakes in British Columbia to identify networks, or "components" , of spatially linked lakes. The motivation for this analysis is principally to inform risk analyses for aquatic invasive species; for example, if a goldfish were released in lake A, what are all of the lakes to which this invasive fish species could potentially travel? Note that this analyses does not currently account for elevation differences between lakes or flow rates.
# ---------------------------------------------------------- #

#Set options, load in libraries to use, and clean the working space.
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(ggthemes)
library(ggspatial)
library(patchwork)
library(rmapshaper)
library(tictoc)
library(bcdata)
rm(list = ls())

#Set your working directory
mywd = "C:/Users/CMADSEN/Downloads/LocalRWork/WaterbodyConnectivity/"
setwd(mywd)
```

```{r load in data}
#Load in the spatial file for British Columbia. I previously used the sf package's function 'st_simplify()' to reduce the file size.
bc = read_sf("bc_simple.gpkg")

#Query the BC Data Warehouse (freely accessible data!) for the subwatersheds of BC (N = 246)
regions = bcdc_query_geodata('freshwater-atlas-watershed-groups') %>%
  collect()
```

```{r set parameters}
#If you run the analysis loop below and it stops after analysis a certain number of subwatersheds (e.g. because of a loss of internet connection, or a query to the BC Data Warehouse has a brain fart), you can update the 'starting_number' object here to resume the analysis at that subwatershed.
starting_number = 1
```

```{r background_plot}
#Set up visual output - this allows us to see the province, its subwatersheds, and which subwatershed the analysis loop is currently working on.
p_grid = ggplot() + 
  geom_sf(data = bc) +
  ggthemes::theme_map()

print(p_grid)
```

```{r analysis loop}
####################################
###       ANALSIS LOOP          !###
####################################

#If you've run this analysis loop already and have partially complete results (e.g. you have results for subwatersheds 1 to 100, and are now going to resume on subwatershed 101), load in those results.

if(file.exists("lakes_network_table.csv")){
  output_table = read_csv("lakes_network_table.csv")
}
 
for(i in starting_number:nrow(regions)){
  
  #Zoom in on the i'th subwatershed.
  region = regions[i,] %>% summarise(subwatershed = WATERSHED_GROUP_ID)
    
  #Update visual interface.
  updated_grid = p_grid + 
    geom_sf(data = region, col = "blue", fill = "lightblue", alpha = 0.5) + 
    labs(title = paste0("Region ",region$subwatershed,", ",i, " of ",nrow(regions)))
  
  print(updated_grid)
  
  #If the region is made up of a large number of separate polygons, remove any that are less than
  #1,000,000 square meters. This is useful for coastal subwatersheds, as it removes any little islands without any waterbodies, which otherwise break the code!
  if(st_geometry_type(region) == "MULTIPOLYGON"){
    region = region %>% 
      st_cast("POLYGON") %>% 
      mutate(area = as.numeric(st_area(.))) %>% 
      filter(area > 1000000) %>% 
      st_cast("MULTIPOLYGON")
  }
      
  #Query and temporarily download rivers and lakes for the i'th subwatershed.
  
  print(paste0("Region ",i," - Downloading rivers and lakes with bcdata package"))

  rivers = bcdc_query_geodata('freshwater-atlas-rivers') %>% filter(INTERSECTS(region)) %>% collect()
  lakes = bcdc_query_geodata('freshwater-atlas-lakes') %>% filter(INTERSECTS(region)) %>% collect()
  
  print(paste0("Region ",i," - Finished data download for this region"))
    
  #Trim away and rivers and lakes that aren't actually inside the i'th subwatershed.
  rivers = st_join(rivers, region, st_intersects) %>% filter(!is.na(subwatershed))
  lakes = st_join(lakes, region, st_intersects) %>% filter(!is.na(subwatershed))
  
  print(paste0("Region ",i," - Cropped rivers and lakes to region outline"))
  
  #Buffer lakes by a little bit (3 meters) to ensure spatial overlap. Otherwise, very small gaps between lakes and rivers that connect in reality may result in the analysis not recognizing these overlaps.
  tic()
  print(paste0("Region ",i," - Buffering polygon of interest to ensure some overlap with connectors."))
  lakes = st_buffer(lakes, dist = 3)
  toc()
  
  print(paste0("Region ",i," - Building networks."))
  
  #Prepare shapefiles for network analysis - reduce the number of columns, add a new column for the number of components (may remove this later).
  lakes = lakes %>% 
    select(WATERBODY_KEY,WATERSHED_GROUP_ID,GNIS_NAME_1) %>% 
    mutate(num_components = 1)
  
  #Strip away all columns except the geometry column for the rivers; remove Z coordinates from any rivers that have them (these indicate height).
  rivers = rivers %>% 
    select(geometry) %>% 
    st_zm()
    
  #Find the number of rivers that connect to lakes.
  number_connections_table = as.data.frame(st_intersects(lakes, rivers)) %>% 
    as_tibble() %>% 
    group_by(row.id) %>% 
    summarise(number_connections = n())
  
  #Add a new variable to the lakes object - this indicates the number of rivers that touch each lake. We initialize the column with 0s for all lakes.
  lakes$num_connections = 0
  
  #Add in the number of connections to the lakes object.
  lakes[number_connections_table$row.id,]$num_connections = number_connections_table$number_connections
  print(paste0("Region ",i," - Found number of connecting rivers/rivers for each lake."))
    
  tic()
  #This is the core element of this analysis loop - for any lakes and rivers that overlap, we join those polygons together into networks.
  print(paste0("Region ",i," - Union and cast polygons to networks..."))
  networks = st_cast(st_union(lakes %>% 
                                bind_rows(rivers)), "POLYGON") %>% 
    st_as_sf()
  toc()
  
  #Find out which lakes from the cropped lake layer are in each network. This enables us to produce the output table that describes which network each lake is part of.
  networks = networks %>% mutate(network_number = row_number(),
                        region = i)
    
  lakes = st_join(networks, lakes, st_intersects) %>% 
    filter(!is.na(WATERSHED_GROUP_ID))
    
  #Remove networks that proved to intersect with NO lakes.
  networks = networks %>% 
    filter(network_number %in% lakes$network_number)
  
  #Save any of the networks that are within 100 meters of the subwatershed edges. WE use these edge networks later on to join together networks that actually cross subwatershed boundaries!
  region_edge = st_difference(region %>% summarise(), st_buffer(region %>% summarise(), -1000)) %>% 
    mutate(region_edge = T)
  
  edge_networks = st_join(networks, region_edge, st_intersects) %>% 
    filter(!is.na(region_edge))
  
  edge_networks$subwatershed = regions[i,]$WATERSHED_GROUP_ID
  
  #Just keep the portions of networks within the band of the region_edge.
  edge_networks = st_intersection(edge_networks, region_edge %>% dplyr::select(-region_edge)) %>% 
    rename(geom = x)
  
  tic()
  if(i == 1){
    #If this is the first round of the loop, establish a geopackage for these edge networks.
    sf::write_sf(edge_networks,paste0("edge_networks.gpkg"),overwrite=T)
  }else{
    #If it's not the first round of the loop, read in the edge network geopackage, add this round's edge networks, and write it back to disk.
    all_edge_networks = read_sf("edge_networks.gpkg")
    all_edge_networks = bind_rows(all_edge_networks, edge_networks)
    sf::write_sf(all_edge_networks,"edge_networks.gpkg",overwrite=T)
  }
  print(paste0("Region ",i," - Edge networks written to disk."))
  toc()
  
  #Drop lake geometry - we save the results in a tabular format (much smaller file!).
  lakes = lakes %>% 
    st_drop_geometry() %>% 
    group_by(WATERBODY_KEY,
             WATERSHED_GROUP_ID,
             GNIS_NAME_1)
  print(paste0("Network IDs assigned to polygons of interest in region ",i))
    
  #Save the results of each loop to a table.
  lakes = lakes %>% 
    mutate(network_number = paste0(WATERSHED_GROUP_ID,"-",network_number))
  
  if(i == 1){
    output_table = lakes[0,] %>% 
      select(-num_components) %>% 
      ungroup() %>% 
      mutate(regions_completed = i)
  }
  
  output_table = bind_rows(output_table, lakes %>% 
                             dplyr::select(-num_components) %>% 
                             ungroup()) %>% 
    mutate(regions_completed = i)
  
  #Write output table to disk.
  write.csv(output_table, 
              paste0("lakes_network_table.csv"),
              row.names = F)
  
  print(paste0("Lake network table updated. Number of non-NA lakes is now: ", nrow(output_table %>% filter(!is.na(network_number)))))
    
  print(paste0("Networks found for region ",i))
}
```


```{r}
### Edge Networks ###

# Some networks abut the boundaries between subwatersheds. These networks may, in reality, be joined. This section is where we fuse together those networks!

# If the above 'analysis loop' section is finished, load in the output_table.
output_table = read_csv("lakes_network_table.csv")
all_edge_networks = read_sf('edge_networks.gpkg')

#First, just to be generous, buffer the edge networks by 2 more meters (total buffer now 3m). This may join together rivers whose geometries don't otherwise come in contact with their 'partners' across the subwatershed boundaries.
#all_edge_networks = st_buffer(all_edge_networks, dist = 2)

#Find which rows overlap which other rows from our all_edge_networks object.
overlap_matrix = st_intersects(all_edge_networks)

overlap_table = overlap_matrix %>% 
  as.data.frame() %>% 
  as_tibble() %>% 
  filter(row.id != col.id)

#Make a table of two columns that lists edge networks that should be joined.
overlaps_to_add_to_output_table = cbind(all_edge_networks %>% 
  st_drop_geometry() %>% 
  slice(overlap_table$row.id) %>% 
  summarise(network_number = paste0(subwatershed,"-",network_number)), 
  all_edge_networks %>% 
  st_drop_geometry() %>% 
  slice(overlap_table$col.id) %>% 
  summarise(network_number_right = paste0(subwatershed,"-",network_number))
) %>% 
  as_tibble() %>% 
  #Drop any rows that are of networks in the same subwatershed.
  filter(str_extract(network_number,"^[0-9]*(?=-)") != str_extract(network_number_right,"^[0-9]*(?=-)"))
  
overlaps_to_add_to_output_table = overlaps_to_add_to_output_table %>% 
  group_by(network_number) %>% 
  summarise(network_number_right = paste0(network_number_right, collapse = ", ")) %>% 
  separate(network_number_right, sep = ", ", into = c(paste0("network_number_right_",rep(1:7))))

#Rename the output_table to 'lake_networks'
lake_networks = output_table

#Add a flag for which lakes need to have their networks merged - for this, we use the table of two columns we made just above.
lake_networks = lake_networks %>% 
  left_join(overlaps_to_add_to_output_table)

#The network IDs are specific to each subwatershed - i.e., every subwatershed has a network 1, 2, 3, etc... also, because network IDs were determined using row_number(), there are many missing numbers. 
#The following table lists the number of unique networks per subwatershed, and gives us a cumulative sum for each
#watershed that can be used as a network ID correction factor!

networks_per_subw = lake_networks %>% 
  group_by(WATERSHED_GROUP_ID) %>% 
  summarise(number_networks_per_subw = length(unique(network_number))) %>% 
  mutate(network_correction_factor = lag(number_networks_per_subw)) %>% 
  mutate(network_correction_factor = replace_na(network_correction_factor, 0)) %>% 
  mutate(network_correction_factor = cumsum(network_correction_factor))

#Replace the 'old' network ID (i.e., the ID specific to each subwatershed) with the province-wide numbering system.
lake_networks = lake_networks %>%
  ungroup() %>% 
  mutate(network_number_old = network_number) %>% 
  mutate(network_number = as.numeric(str_remove(network_number, "[0-9]*-"))) %>% 
  group_by(WATERSHED_GROUP_ID, network_number) %>% 
  mutate(network_number_new = cur_group_id()) %>% 
  select(network_number,network_number_new,everything()) %>% 
  #Add in the table showing the number of unique components in each subwatershed.
  left_join(networks_per_subw) %>% 
  mutate(network_number = network_number_new + network_correction_factor) %>% 
  select(-network_number_new) %>% 
  ungroup()

network_number_old_to_new = lake_networks %>% 
  dplyr::select(network_number,network_number_old,network_number_right_1) %>% 
  distinct() %>% 
  filter(!is.na(network_number_right_1)) %>% 
  dplyr::select(-network_number_right_1)

# The following code is janky and should not be used as a learning example! :P
# We need to look across 7 columns that contain the (old) network IDs of overlapping edge networks.
# With this, we can replace the old IDs of those networks with the new network IDs.
# This table output lists, for overlapping edge networks, the network number, the list of networks in the overlapping segment (NOTE: can be multiple overlaps that include a given network!), and what the lowest network in that overlapping segment is.
lake_network_replace_table = lake_networks %>% 
  dplyr::select(network_number_old,starts_with("network_number_right_")) %>% 
  distinct() %>% 
  filter(!is.na(network_number_right_1)) %>% 
  left_join(network_number_old_to_new) %>% 
  mutate(network_number_old = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_1 = network_number_old)) %>% 
  mutate(network_number_right_1 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_2 = network_number_old)) %>% 
  mutate(network_number_right_2 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_3 = network_number_old)) %>% 
  mutate(network_number_right_3 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_4 = network_number_old)) %>% 
  mutate(network_number_right_4 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_5 = network_number_old)) %>% 
  mutate(network_number_right_5 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_6 = network_number_old)) %>% 
  mutate(network_number_right_6 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  left_join(network_number_old_to_new %>% rename(network_number_right_7 = network_number_old)) %>% 
  mutate(network_number_right_7 = network_number) %>% 
  dplyr::select(-network_number) %>% 
  #Now each column has had its network's ID updated to the cross-subwatershed numbering system.
  rename(network_number = network_number_old) %>% 
  pivot_longer(-network_number) %>% 
  filter(!is.na(value)) %>% 
  group_by(network_number) %>% 
  summarise(networks_to_be_grouped = paste0(value, collapse = ", "),
            lowest_network_in_group = min(c(network_number,value), na.rm=T)) %>% 
  mutate(networks_to_be_grouped = paste0(network_number, ", ", networks_to_be_grouped))

#Finally replace those old IDs!! 
lake_networks = lake_networks %>% 
  dplyr::select(-starts_with("network_number_right_")) %>% 
  left_join(lake_network_replace_table) %>% 
  #mutate(networks_to_be_grouped = str_extract_all(networks_to_be_grouped, "[0-9]")) %>% 
  mutate(network_number = case_when(
    #If there are no other networks in this group to be merged, keep network number as is.
    is.na(networks_to_be_grouped) ~ network_number,
    #If the 'networks_to_be_grouped' field isn't NA, use the lowest network ID to replace this row's network_number.
    T ~ lowest_network_in_group
  )) %>% 
  dplyr::select(-regions_completed,
                -network_number_old,
                -network_correction_factor,
                -networks_to_be_grouped,
                -lowest_network_in_group)

#Also, some lakes span more than one subwatershed (e.g. Williston Lake). 309 rows, or 28 lakes total.
#The networks that these lakes are part of can be joined!
border_lakes = lake_networks %>% 
  mutate(identifier = paste0(WATERBODY_KEY,GNIS_NAME_1)) %>% 
  filter(identifier %in% all_of(lake_networks %>%
                                  filter(!is.na(GNIS_NAME_1)) %>% 
                                  mutate(lake_id = paste0(WATERBODY_KEY,GNIS_NAME_1)) %>%
                                  filter(duplicated(lake_id)) %>%
                                  summarise(identifier = paste0(WATERBODY_KEY,GNIS_NAME_1)) %>% 
                                  distinct() %>% 
                                  pull(identifier))) %>% 
  group_by(WATERBODY_KEY,GNIS_NAME_1) %>%
  mutate(lowest_network_id = min(network_number),
         networks_to_merge = paste0(unique(network_number), collapse = ",")) %>%
  ungroup() %>% 
  filter(str_detect(networks_to_merge, ","))

lake_networks = lake_networks %>%
  left_join(border_lakes %>%
              dplyr::select(network_number,
                            networks_to_merge,lowest_network_id) %>%
              distinct()) %>%
  distinct() %>% 
  mutate(network_number = case_when(
    #If there's nothing in the 'lowest_network_id' field, retain the number that was in 'network_number'!
    is.na(lowest_network_id) ~ network_number,
    T ~ lowest_network_id
  )) %>% 
  dplyr::select(-region,-networks_to_merge,-lowest_network_id)
```

We now have a table the includes every lake in BC, with an added field that identifies the component network that each lake is a part of. The networks were initially delineated by connecting all lakes and rivers within 4 meters of each other (lakes by 3 m). These networks were combined with intersecting networks of other subwatersheds. Finally, some lakes straddle the boundaries between subwatersheds - we found all such lakes THAT ARE NAMED (for safety's sake in terms of merging... we want to avoid incorrectly merging unnamed lakes that are harded to differentiate between because of their lack of a name) and that share a waterbody_key (quite reliable identifier for lake polygons that are separate but that refer to the same lake in the real world) and assigned whatever the lowest network ID of each set of matching lake polygons was to ALL of the lakes in those networks (i.e. the lake functions as a connector for the networks).

```{r output results}
write.csv(lake_networks, "provincial_waterbody_networks.csv", row.names = F)
```
